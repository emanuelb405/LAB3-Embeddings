# -*- coding: utf-8 -*-
"""Embedding_word2vec_sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1efuxMCvMoG0ck5tszHDcik61LgvusBI2

# Word2Vec
"""

from string import punctuation
from os import listdir
from numpy import array
from numpy import asarray
from numpy import zeros

# load file into memory
def load_file(path):
  import string
  
  file = open(path, 'rt')
  text = file.read()
  file.close()
  #split into sentences at fullstop
  sentences = text.split('.')
  
  return sentences

#remove everything from a sentence that is not in the vocab dictionary
def clean_sentences(path,vocab):
  import string
  cleaned_sentences = list()
  sentences = load_file(path)

  for sentence in sentences:
    
    
    words = sentence.split()
    # remove punctuation from each word
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in words]
    stripped = [word.lower() for word in stripped]
    
    
    clean_sentence = list()
    for word in stripped:
      if word in vocab:
        clean_sentence.append(word)
      else:
        continue
    cleaned_sentences.append(clean_sentence)
  return cleaned_sentences

#iterate over all files in directory and extract the sentences from them
def extract_sentences(directory,vocab):
  sentences = list()
  for filename in listdir(directory):
    path = directory + '/' + filename
    clean_sent = clean_sentences(path,vocab) 
    sentences+=clean_sent
  return sentences

#load vocab
file = open('files/vocab.txt', 'r')
vocab = file.read()
file.close()

neg_train = extract_sentences('aclImdb/train/neg', vocab)
pos_train = extract_sentences('aclImdb/train/pos', vocab)

neg_test = extract_sentences('aclImdb/test/neg', vocab)
pos_test = extract_sentences('aclImdb/test/pos', vocab)

sentences = neg_train+pos_train+neg_test+neg_test

from gensim.models import Word2Vec

# train word2vec model
model = Word2Vec(sentences, size=100, window=5, workers=8, min_count=1)
# summarize vocabulary size in model
words = list(model.wv.vocab)
print('Vocabulary size: %d' % len(words))

acc = model.wv.accuracy('files/questions-words.txt')

#google released test file for testing woman + king = queen
#https://raw.githubusercontent.com/RaRe-Technologies/gensim/develop/gensim/test/test_data/questions-words.txt
import pandas as pd
df = pd.DataFrame(['correct','incorrect'],columns=['type'])
for entry in acc:
  section = entry['section']
  
  correct = len(entry['correct'])
  incorrect = len(entry['incorrect'])
  pos = correct/(correct+incorrect)*100
  neg = incorrect/(correct+incorrect)*100
  df[str(section)] = (pos,neg)

df.to_csv('results/embeddings_on_google_test_set.csv')

# save model in word2vec format
filename = 'models/embedding_word2vec_.txt'
model.wv.save_word2vec_format(filename, binary=False)

#display the word embedding in 2D picture through dimensionality reduction
# %matplotlib inline
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE


# Python code to convert into dictionary 
  
def Convert(tup, di): 
    for a, b in tup: 
        di.setdefault(a, []).append(b) 
    return di 

#define closest to which word you want to extract
word = 'bad'
      
# Driver Code     
tups = model.most_similar(word,topn=50)
dictionary = {} 

dictionary = Convert(tups, dictionary)


wanted_keys = list(dictionary.keys())
wanted_keys.append(word)

vocabulary = model.wv.vocab

sub_dict= {k:v for k,v in vocabulary.items() if k in wanted_keys}


#X = model[model.wv.vocab]

X = model[sub_dict]
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(X)

fig, ax = plt.subplots()

ax.scatter(X_tsne[:, 0], X_tsne[:, 1])

for i, txt in enumerate(wanted_keys):
    ax.annotate(txt, (X_tsne[i, 0], X_tsne[i, 1]))

ax.set_title(str("Closest to "+word))

