# -*- coding: utf-8 -*-
"""train_cnn_no_textcleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lpsfp5NBh70rVkaWGeD7Bfh89iLFzfOT

# Train without any preprocessing and no pretrained embedding layer
"""

import pandas as pd

train = pd.read_csv('files/train.csv')
test = pd.read_csv('files/test.csv')

X_train = train['text'].values
X_test = test['text'].values
y_train = train['sentiment'].values
y_test = test['sentiment'].values

from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences


tokenizer = Tokenizer()
total_reviews = X_train + X_test
tokenizer.fit_on_texts(total_reviews) 

# pad sequences
max_length = max([len(s.split()) for s in total_reviews])

# define vocabulary size
vocab_size = len(tokenizer.word_index) + 1

#tokenize all the input data
X_train_tokens =  tokenizer.texts_to_sequences(X_train)
X_test_tokens = tokenizer.texts_to_sequences(X_test)

#create paddings with the same length
X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')
X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')

from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, CSVLogger, ReduceLROnPlateau

checkpoint = ModelCheckpoint(
    'models/best_cnn_no_textcleaning.model',
    monitor='val_loss',
    verbose=1,
    save_best_only=True,
    mode='min',
    save_weights_only=False,
    period=1
)
earlystop = EarlyStopping(
    monitor='val_loss',
    min_delta=0.001,
    patience=30,
    verbose=1,
    mode='auto'
)

csvlogger = CSVLogger(
    filename= "results/cnn_no_textcleaning_training_csv.log",
    separator = ",",
    append = False
)

reduce = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.1,
    patience=3,
    verbose=1, 
    mode='auto'
)

callbacks = [checkpoint,csvlogger,reduce]

from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten
from keras.layers.embeddings import Embedding

emb_dim = 100  #embedding dimension

# define model
model = Sequential()
model.add(Embedding(vocab_size, emb_dim, input_length=max_length))
model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
print(model.summary())
# compile network
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# fit network
model.fit(X_train_pad, y_train, batch_size=128, epochs=20, 
          validation_data=(X_test_pad, y_test), verbose=1,callbacks=callbacks)
# evaluate
loss, acc = model.evaluate(X_test_pad, y_test, verbose=0)
print('Test Accuracy: %f' % (acc*100))

from keras.utils.vis_utils import plot_model
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

train[train['sentiment']==0].head()

test[test['sentiment']==0].head()

